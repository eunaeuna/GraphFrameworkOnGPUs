Achieving good run-time performance for a graph algorithms is challenging. %Period.
The execution time is dependent on a wide range of parameters: type of system (shared-memory vs.\ distributed), number of threads (from tens of threads on CPU systems to tens of thousands on current GPU processors), algorithm and problem statement (BLAS, vertex centric, GAS, BSP), load-balancing, and the input graph itself.
Given these complexities, programming frameworks which can encompass and abstract away many of these programming challenges will have the benefit of being accessible to a wider range of programmers and users. Additional benefits of frameworks include improved user productivity, increased code portability, and additional functionality (that can also include basic ETL functionality such as loading and storing a graph).

Over the years, various frameworks have been developed across a broad range of languages and parallel frameworks, including: OpenMP MPI, Cilk, C++ AMP (Accelerated Massive Parallelism) or CUDA.  Other approaches include Intel's TBB and  Kokkos~\cite{edwards2014kokkos} which give the user a set of basic primitives to implement their algorithms. However, these frameworks tend to require a good amount of expertise and do not offer many implementations.



\subsubsection{Shared Memory vs. Distributed Memory Systems}
Over the last two decades there has been in an increase in the number of scalable graph frameworks. These have primarily split into two types of frameworks, the distributed memory frameworks and the shared memory framework. While there are several frameworks that can work on both systems (such as Kokkos~\cite{edwards2014kokkos}), they are typically designed for one type of system.  Furthermore, these frameworks are designed to solve different types of problems. For example, PowerGraph~\cite{powergraph}, Pregel~\cite{pregel}, and GraphX~\cite{gonzalez2014graphx} which are all widely used distributed frameworks currently support a small subset of graph problems, which fall under the category of algorithms that can be implemented with bulk synchronous operation. This makes the algorithms very communication bound and limits these graph frameworks ability to support computationally intensive and IO intensive graph algorithms due to their random (sometimes referred to as irregular) memory access pattern.

In contrast, shared memory graph frameworks such as LIGRA~\cite{shun2013ligra} and  GAP~\cite{beamer2015gap} can support a wider range of algorithms however are limited to the computational resources and memory of a single compute node. GAP, unlike LIGRA, requires lower programming and does not abstract away the HPC details of its framework.
GPU graph frameworks fall into this category as well and offer significantly higher computational resources at the cost of reduced memory subsystem.
These frameworks include (to name a few): Gunrock~\cite{wang2017gunrock}, Hornet~\cite{green-hornet} (and cuSTINGER~\cite{DBLP:conf/hpec/GreenB16}, cuGraph (formerly nvGraph~\cite{nvGraph}), and cuSHA~\cite{cusha}.
Gunrock and Hornet use a vertex centric programming model. nvGraph's programming model is based on GraphBLAS (an extension of BLAS designed for graphs). cuSHA uses a (Gather-Apply-Scatter) GAS programming model.
%\cite{green-custinger}

\subsection{Graph Frameworks Functionality}

Given the challenges of implementing high performance and scalable graph algorithms, there is clear need for higher level functionality that abstracts away the key functionality of graph algorithms from the implementation on actual compute systems. This topic has received a good amount of research over the last decade and several paradigms have been designed. While these frameworks have a good amount of similarity, they are also quite different. One thing that they share in common is that they require the programmer to ``think'' in their programming paradigm.


LIGRA~\cite{shun2013ligra} and Gunrock~\cite{wang2017gunrock} use frontier based approach that suggests that parallelism be visualized as ``waves'' of vertices or edges that can be traversed concurrently. In practice, these frontiers are created explicitly by the user using very simple and scalable filtering functionality . Then, all vertices and edges in the frontier can be traversed concurrently. Thus, these frontier operations consist of two phases. Hornet~\cite{green-hornet} takes a slightly different approach than LIGRA and Gunrock and offers a set of primitives (sometimes referred to as \emph{parallel-for} loops) that operates on sets of vertices and edges. Specifically, in Hornet the edge lists are created explicitly by the framework and implicitly by the user. Thus, the frontier-like operations occur in one phase rather than two phases.

Galois~\cite{pingali2011tao} uses an operator based formulation that separates the compute operation from the actual parallel runtime. Thus, Galois is responsible for the runtime and dispatches the various work units to the threads.



GraphMat~\cite{Sundaram:2015:GHP:2809974.2809983}, CombBLAS~\cite{Buluc:2011:CBD:2076556.2076566}, and nvGraph~\cite{nvGraph} use BLAS operations to implement graph algorithms. Specifically, graph algorithms are implemented using a small subset of linear algebra operations.
The benefit of this approach is that improvements made to SpGEMM or SpMV immediately improves the performance of the graph algorithms. This also allows for improving scalability of BLAS based solutions; specifically as these operations also receive significant attention for distributed systems and these solutions can they be applied to the graph problem.
The key downside of this approach is that not all problems can be formulated as BLAS operations.



Gather-Apply-Scatter (GAS) is also very popular due to its simple three phase bulk synchronous programming model. Each phase in the algorithm consists of three sub-phases: 1) each vertex receives a set of messages from its neighbors (Gather), 2) each vertex processes the messages (Apply) and 3) each vertex sends a message to its neighbor after process the previous rounds of messages (Scatter). The simplicity of this model makes it very attractive for distributed environments (PowerGraph~\cite{powergraph}, Pregel~\cite{pregel}, and GraphX~\cite{gonzalez2014graphx}). The GAS programming model introduces a large number of synchronization as well a good amount of communication (to send and receive messages) that limits it applicability to a wider range of applications.
